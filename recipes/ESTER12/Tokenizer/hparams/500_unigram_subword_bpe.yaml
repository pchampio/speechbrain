# ############################################################################
# Tokenizer: subword BPE with unigram 500
# Authors:  Pierre Champion
# ############################################################################

output_folder: !ref results/500_subword_unigram_LM/

# Data files
data_folder: !PLACEHOLDER # e.g, /path/to/corpus (**/*.stm)
stm_directory: !ref <data_folder>/**/[^\.ne_e2\.|\.ne\.|\.spk\.|part\.]*.stm
# For more training dataset
# tr_splits: [/ESTER2/train/, /ESTER2/train_trans_rapide/, /ETAPE/train, /EPAC/train]
# te_splits: {"test_ESTER2":["/ESTER2/test/*"], "test_ESTER1":["/ESTER1/test/*"]}
tr_splits: [/train/, /train_trans_rapide/]
dev_splits: [/ESTER2/dev/]
te_splits: {"test_ESTER2":["/ESTER2/test/*"]}
prep_save_folder: !ref <output_folder>
skip_prep: False

# Training parameters
token_type: unigram
token_output: 500  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: text


tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <output_folder>/train.csv
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type>
   character_coverage: !ref <character_coverage>
   annotation_list_to_check: [!ref <output_folder>/train.csv, !ref <output_folder>/dev.csv]

